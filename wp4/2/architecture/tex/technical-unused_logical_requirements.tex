In this section we describe the deployment options from the perspective of
systems and applications, and discuss implications for both applications and
data centres.

\subsection{System Configuration}


\subsection{Application Configuration}


This section describes the implications on a existing parallel application that uses one of the supported interfaces such as NetCDF4/HDF5.
The semantics of the API calls will change slightly but mostly in a way that is backwards compatible.



\subsection{Concurrency semantics}

In general, the system is designed for coordinated parallel applications that access data independently of each other.
That means, within one parallel (MPI) application, some kind of coordination happens to allow the shared access to containers, variables and shards.

Data sharing between independent applications is intended to happen after applications completed.
It is not allowed that multiple currently running parallel applications write data to the same variable.
This is considered to be sufficient for most scenarios, e.g., a model produces some output, once the model completed, the produced data is post-processed.
If an application uses the write-only mode to append data to a variable, the concurrent read-only access of the same or independent applications will not immediate see the new data.
Instead, ESD ensures that all changes to the data (and metadata) becomes atomically visible when the logical container is closed by the application.
At this point, all the new data becomes immediately visible to the readers.

Especially in NWP applications it is useful to access intermediate results and start post-processing as soon as possible to minimise the overall delay.
To support this scenario, we provide the concept of Epoch-Semantics to the developers (see below).

Writing data follows the expected semantics for writes but when multiple writers write data to the same variable coordinates, i.e., they overwrite data, the result is undefined.
In fact, applications in which multiple processes write to the same region are considered to be wrong.

Reading data that is available in the container but has been written by a previously terminated application will be returned in the correct fashion.
Reading data that has been written by the same application (i.e., reads after writes) should not happen, as users should use means of communication inside the application to prevent this kind of access pattern.
If such a fine-grained update of data is necessary, again the Epoch-semantics must be used.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Implications on the User}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Implications on the data store/data services}

\subsubsection{Data Processing}

One of the common themes of research in storage has been to bring specific types
of processing closer to the storage. Obvious first steps in storage-side
processing is checking basic data integrity (did we get all the bytes, and in
the right order as well as updating metadata systems (we have a new file).
t The next steps were to automate metadata extraction in a file-type
dependent way, so a process would recognise the file type and apply
file-specific checks on its structural integrity (is this a well formed
HDF5/PDF/Word document?).  These types of checks are widely used in production
infrastructures.

The next obvious extension to this was twofold: one proposed direction would
bring higher level features closer to the device, to remove the load on
file systems.  For example, in conventional magnetic hard drives one originally
had to know the number of cylinders, sectors, and so on, but LBA vastly
simplified supporting hard drives without having to reconfigure the operating
system (or file system), and in a similar vein, hard drives can do their own
checking and management of bad blocks.

\todo{It may be worth investigating this further, as LFTS could provide similar benefits for tape?
  or are those benefits irrelevant for the purposes we are discussing here?}

A research extension by SNIA proposed drives that provided direct object storage with metadata, thus bringing some of
the work required by the file system into the storage device; the flip side is that the interface would not be full POSIX
but more like an object store.  This particular research, however, never came to production, and we must conclude that
there were not sufficient requirements for it to have a market.

The other direction was in storage management services that would implement programmable storage side workflows, such as
iRODS.  Generally microservices would need to be added in order to support new file types and change the rules, but the
system would be customised to provide relatively simple workflows which were used to manage metadata but could be
extended to do server side processing.

Grids provide similar features but in a different way, as there is technically no difference between storage-side
processing and user-side processing: they are done using the same mechanisms, but essentially in different data
centres. WLCG operates a tiered model where data is generated at a Tier 0 centre, copied to Tier 1 centres whence it is
copied to Tier 2s (these should not be confused with data centre tiers which are essentially the other way around.)  Raw
processing of data ---  what we would call storage side processing --- is done at the Tier 0 and 1s, with no end user access
at all. End user analysis of data would happen at Tier 2s and 3s, and data which need archiving would be copied back to
the local T1 [diagram]

An alternative approach is to treat the storage as a basic service --- like an
object store, typically --- and let higher level workflow engines manage the
writing and processing of the data as a part of a (loosely coupled) workflow.
This approach would also work well provided it can get notifications back from
the store.  Indeed, CASTOR was originally written on top of LSF, but LSF was
removed (around 2010?) in favoured a bespoke transfer manager when it was found
that original versions of CASTOR didn't scale well with the WLCG
workloads.

(A novel approach is to treat the data structure transparently within the applications, without the requirement to
integrate the data management in the application; see future directions\todo{JJ: check what is available}


\subsubsection{Staging and Caching}

State of the art: ERADAT, the BNL extension of (90PB) HPSS [Yu].  The main question is whether every recall can always
be automated or at which stage would it need operator intervention in order to me maximally efficient?

State of the art at NERSC CORI (2017?) [Dosanjh], NVRAM burst buffers; 28 PB secondary disk (Lustre) supporting data
placement close to the CPU; thus a further staing process is required anyway
(cf. diagram) Also JASMIN's PANASAS
with 15PB (using ET for temporarily staging data in order to free up fast disk)

Note the follow-up requirement on meta-operations on data, e.g. staging, pinning.

\todo{NUMA?}

Is it necessary to generate models for this? Is it useful?  For example, the allocation of drives to each individual
user community, and optionally prioritising recalls.

Could the same be applied for other types of file operations that are scheduled, such as replication and integrity
checking.  It would be useful to do opportunistic operations: if a tape (resp. collection) is mounted
(resp. fetched/unpacked), take the opportunity to act on other collections (files/objects), instead of waiting for their
scheduled operation.
