

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Adaptive Data Mappings}
The typical storage mapping for scientific data format libraries is the file (linear sequence of bytes organized following the POSIX file system representation, i.e. inodes and blocks). Information is translated into a linear array of bytes in the file system using appropriate schemas. Since the data model defined by the data format library can contain complex hierarchies and attributes besides raw data, the final file will contain additional scientific metadata that needs to be accessed using the POSIX-IO data interface (e.g. \texttt{read()} and \texttt{write()}) instead of the file metadata interface (e.g. \texttt{stat()}, \texttt{lookup()}). This makes the storage model adopted by data format libraries incompatible with the typical parallel file system organization, in which metadata and data are splitted apart and assigned to different services for optimal performance. Additionally, new storage system paradigms have emerged in the last years in which files are organized in a flat namespace (e.g. object storage), removing the restrictions imposed by metadata operations like namespace traversal of POSIX file systems. Hierarchical organization can be still achieved using other dedicated storage representations like key-value stores, at the expense of reduced POSIX semantic.

There is therefore a necessity for more flexible data mappings that can take advantage of an increasing number of storage and backend alternatives, improving access efficiency at the same time. There are two different approaches to this problem: the first is to develop a new data mapping schema for every storage backend. This is the solution adopted by the HDF5 library (through the virtual file and the virtual object layers) in which multiple storage backends can be employed by developing a corresponding plugin that contains the right mapping schema. The obvious limitation of this solution is that once the user has selected a certain backend for a file, this cannot be changed without migrating the whole file to another backend. The second approach is more flexible and consists in making the storage backend selection adaptive. This has the advantage of enabling backend storage selection on the fly depending upon the type of data being stored or a set of user/system defined parameters (e.g. list of data placement policies satisfying certain requirements of quality of service).

Our proposed solution is to integrate the adaptive data mappings capabilities just discussed in the layout component of the ESD middleware. The middleware will understand the scientific metadata of other relevant data format libraries and will be able to use multiple storage backends at the same time to store and retrieve different pieces of data. To support multiple formats the datatype component in the ESD middleware will expose a datatype interface similar to the one available in HDF5 (described in Section~\ref{sec: data-formats}). Additionally, the middleware will add significant metadata to be used in the life cycle management and sharing of data. This can include semantic metadata describing which data is needed and how it is going to be used, the required level of resilience, etc. In this context, some metadata can be automatically generated by the middleware or defined by the user and passed to the middleware through an apposite interface (that will be part of the ESD exposed interface).

Legacy codes will have access to data using a familiar POSIX interface exported through an ESD FUSE module. The FUSE module will communicate with the ESD middleware to access data and export it to users using a namespace representation. The mapping schema definition for mapping data between the storage representation and the namespace will be done later in the project.

Another important aspect to consider when talking about adaptive data mappings is the storage tiering, that is, how many levels of storage there are in the system. Historically, HPC applications have relied on parallel file systems as first tier to store and retrieve their data. Besides the parallel file system, most high-end clusters have a second level of archival storage to which data is migrated from the parallel file system using a hierarchical storage manager. Nowadays compute nodes have access to local storage (typically a block device formatted with a local file system) and with the emergence of new storage technologies, such as non-volatile memories, permanent byte addressable memory. The ESD middleware should be able to exploit these local storage resources to implement prefetching strategies (read patterns) and burst buffers (write patterns).

%In the specific case of Mero (Seagate object store system), which represents one of the possible storage backends that has to be supported by the ESD middleware, hierarchical and semantic metadata can be stored in the key-value store service while array data can be stored into objects. In fact, we can think about using more than one object per dataset. This is particularly useful with chunking since every chunk in the dataset can be mapped to a different object, making parallel access to different data units possible without any need for I/O coordination (no false sharing is possible in this case). Of course, I/O coordination still needs to be provided when multiple processes access the same dataset and chunking is not enabled. In this case the ROMIO library already provides a collective I/O implementation that can be reused by the ESD middleware. Whenever needed, false sharing of data can also be avoided by using `persistent file realm' like mechanisms\footnote{This mechanism allows a certain file portion to be assigned to a specific process in the application and makes this assignment persistent across multiple data accesses.}.

%\todo{Giuseppe: you had a very good document on this already can add something here}

%Goal of the dynamic mapping is to create a hierarchical namespace based on the available metadata.
%It is based on the search for objects.
%With the help of FUSE, these mappings can be created based on a user configuration upon mount-time (supports user mounts).
%Inaccessible data, i.e., where the permissions are not sufficient, can be hidden from the namespace.

%An example mapping could be:
%âv.info.model/v.info.environment.date/v.info.experiment.tags/v.info.name <as NC4>â.
%This would show a single NetCDF4 file for each variable.
%If a variable is attached to various tags, then it is shown for each of them.

%âv.info.model/v.info.environment.date/v.domain.time/v.info.name <as NC4>â.
%Would show one file per timestep.

%How to resolve ambiguity?

%Existing containers can also be directly mapped, showing the input and output variables similarly:
%â<c.\_id>/<c.directory>â


%The storage should now how the logical data space maps to physical positions in memory or storage.
%If the storage backends have this knowlege, they can run local operations on the data.

%Alternatives:
%Triangular grid (locally refined), somehow mapped from user-space to a 1D structure which gets mapped to the storage. (Traditional approach)

%Application(data structure) - Mapping - Middleware - Mapping (byte array) - Storage API - Storage backend

%or:

%Application - Middleware - Storage API - Mapping - Storage backend

%Use sth. like MPI datatypes to descripe memory (in the application)
%Add metadata to describe the meaning / semantics, additional gain: resilience because we know which data to replicate to put into triple ECC ...
%Add hints how the data is used / will the used in the future

%Avoid redundant descriptions of memory / storage.
%Users stores a 3D field, it is clear which storage location is responsible for [x,y,z]
%How to store triangles etc.

%Why not store code as description?
%- describes neighborhood?

%Coordinates? Universal coordinates, application coordinates?

%VTK? haben diverse Datenstrukturen hierfÃŒr.


%Separation of concerns...

%Missmatch of Chunks size and application domain

%false sharing

%Pointer Datentyp: strong ptr, weak ptr.
%like Boost usw.


%NVRAM



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Technical Metadata}
\label{subsec: technical metadata}

Besides scientific metadata, the dynamic mapping of data to storage backends requires further metadata that must be managed.
To distinguish technical metadata from scientific metadata, an internal namespace is created.
Relevant metadata is shown in \Cref{tbl:additionalTechnicalMetadata} for shards, variables and containers, respectively.

Metadata can be optional (O) or mandatory (M), and either is created automatically or must be set manually via the APIs.
Automatic fields cannot be changed.
Some of the data can be automatically inferred if not set manually, but manual setting may allow further optimizations.

Some of the metadata is used on several places, for example, information about the data lineage might be used to create several output variables.
In our initial implementation, the metadata is stored redundantly as this:
1) simplifies search; 2) enables us to restore data on corrupted storage systems by reading the metadata; 3) reduces contention and potentially false sharing of metadata.
An implementation might decide to reduce this by utilizing a normalized schema.

References is the list of objects that are directly used by this object, e.g., other variables that are used to define the data further.

\begin{table}
\begin{subtable}[t]{\textwidth}
\begin{tabular}{llll}
Metadata & Field & Creation & Description\\
\hline
Domain   & M & Auto & The subdomain this data covers from the variable\\
Type     & M & Auto & The (potentially derived) datatype of this shard\\
Variable & M & Auto & The ID of the variable this data belongs to\\
Storage  & M & Auto & The storage backend used and its options\\
References & M & Auto & A list of objects that are referenced by this data\\
Sealed   & M & Auto & A sealed shard is read-only\\
\end{tabular}
\caption{For a shard}
\end{subtable}

\begin{subtable}[t]{\textwidth}
\begin{tabular}{llll}
Metadata & Field & Creation & Description\\
\hline
Domain      & M & Manual & Describes the overall domain\\
Type   	    & M & Manual & The (potentially derived) datatype\\
Info   	    & M & Manual & The scientific metadata of this document\\
References  & M & Auto & A list of objects that are referenced by this data\\
Permissions & M & Auto/Manual & The owner and permissions \\
Shards      & M & Auto & The list of shard objects for this variable\\
Sealed      & M & Auto & A sealed variable is read-only\\
\end{tabular}
\caption{For a variable}
\end{subtable}

\todo{
	6.X Mero => TODO Seagate
	6.X HDF5+MPI plugin
	* Typical run as MPI+HDF5 application ?
	** Master process, setup files?
	** Beispiel: Workflows using the containers => aus dem Use Case
	"Pipelines/Workflows"
}

\begin{subtable}[t]{\textwidth}
\begin{tabular}{llll}
Metadata & Field & Creation & Description\\
\hline
Owner    & O     & Manual   & The owner of this file view (see the permission model)\\
Info     & O     & Manual   & Additional scientific metadata for this view\\
Directory & O    & Manual   & Contains a mapping from names to variables\\
Environment & O  & Automatic & Information about the application run\\
Permissions & M & Auto/Manual & The owner and permissions \\
References  & M & Auto & A list of objects that are referenced by this data.
\end{tabular}
\caption{For a container}
\end{subtable}
\caption{Excerpt of additional technical metadata}
\label{tbl:additionalTechnicalMetadata}
\end{table}


\paragraph{Example}

This example illustrates data of a predictive model could be stored on the system and the resulting metadata.
The dimensionality of the underlying grid is fixed.

The application uses the following information to drive the simulation:
\begin{itemize}
 \item Timerange: the simulated model time (from a starting datetime to the specified end)
 \item Longitude/Latitude: 1D data field with the coordinates [float]
 \item Temperature: Initial 2D field defined on (lon, lat)
\end{itemize}
A real model would use further parameters to estimate the temperature but these are sufficient to demonstrate the concepts.
This information is either given as parameter to the simulation or read from an input (container).
A mixture of both settings is possible.


The application produces the following output:
\begin{itemize}
  \item Longitude/Latitude: 1D data field with the coordinates [float]
  \item Model time: the current time inside the simulation
  \item Temperature: 2D field defined on (lon, lat, time) [float], containing the precise temperature on the coordinates defined by lon and lat for the given timestep
  \item AvgTemp: 1D field defined on (time) [float]; contains the mean temperature for the given time
\end{itemize}


Upon application startup, we create a new virtual container that provide links to the already existing input.
In \Cref{lst:mongoContainer}, the metadata for the container is shown, after the application is started.
We assume it has used the APIs to provide the information (input, output, scientific metadata).
In this example, we explicitly define the objects used as input; it is possible to also define
the input as an already existing container.
It is also possible to define the input a-priori if the objectIDs are known / looked up prior application run.
The intended output variables could be given with their rough sizes.
This would allow the scheduler to pre-stage the input and ensure that there is enough storage space available for the output.
The environment information is inferred to the info object but can be changed from the user.

\begin{tcbcode}[label={lst:mongoContainer}]{JSON document describing the container}
\begin{lstlisting}
"_id" : ObjectId(".."),
"directory" : {
  "input" : {
    "longitude" : ObjectId(".."),
    "latitude" : ObjectId(".."),
    "temperature" : ObjectId("..")
   },
  "output" {
     "temperature" : ObjectId(".."),
     "avgTemp" : ObjectId("..")
   }
},
"info" : {
  "model" : { "name" : "my model", "version" : "git ...4711" },
  "experiment" : {
    "tags"        : ["simulation", "poisson", "temperature"]
    "description" : "Trivial simulation of temperature using a poisson process"
  },
},
"environment" : {
  "date"  : datetime(2016, 12, 1),
  "system" : "mistral",
  "nodes" : ["m[1-1000]"]
},
"permissions" : {
  "UID"  : 1012,
  "GID"  : 400,
  "group" : "w", # allows read also
  "other" : "r"
},
"references" : {
  [ all links to used object IDs from input / output ]
}
\end{lstlisting}
\end{tcbcode}

The metadata for a single variable is build based on the information available in the container and additional data provided by the user.
An example for the temperature variable is shown in \Cref{lst:mongotemperature}.
When describing the domain that is covered by the variable, there are two alternatives:
1) a reference to an existing variable is embedded and the minimum / maximum value is provided.
This allows to reuse descriptive information as data has to be stored only once. Min and max describe the multidimensional index of the subdomain in the variable that is actually referenced;
2) data becomes embedded into the file. This option is used when the size of the variable is small.

An advantage of option 2) is that searches for data with a certain property do not require to lookup information in additional metadata.

Similarly, information about the data lineage (history) can originally be inferred from the objects linked in the directory mapping.
The metadata of the referenced object must be copied, if the original object is removed.

\begin{tcbcode}[label={lst:mongotemperature}]{JSON document for temperature}
\begin{lstlisting}
"_id" : ObjectId("<TEMPID>"),
"sealed" : true,
"domain" : [
    "longitude" : [ "min" : 0, "max" : 359999, "reference" : ObjectId("..") ],
    "latitude" : [ "min" : 0, "max" : 179999, "reference" : ObjectId("..") ],
    "time" : [ datetime(...), datetime(...), ... ]
  ],
"type" : "float",
"info" : {
  "convention" : "CF-1.0",
  "name" : "temperature",
  "unit" : "K",
  "long description" : "This is the temperature",
  "experiment" : {
    "tags"        : ["simulation", "poisson", "temperature"]
    "description" : "Trivial simulation of temperature using a poisson process"
  },
  "model" : { "name" : "my model", "version" : "git ...4711" },
  "directory" : {
	"input" : {
	  "longitude" : ObjectId("<LONID>"),
	  "latitude" : ObjectId("<LATID>"),
	  "temperature" : ObjectId("<TEMPID>")
	}
  },
  "environment" : {
    "date"    : datetime(2016, 12, 1),
    "system" : "mistral",
    "nodes"  : ["m[1-1000]"]
  },
  "history" : [
  To illustrate the applied mapping, we use a subset of our NetCDF metadata described in \Cref{sec:netcdfDataMapping}.
  The excerpt is given in \Cref{lst:NetCDF-data-map}.
  The mapping of a single logical variable is exemplarily described in


  \begin{tcbcode}[label={lst:NetCDF-data-map}]{NetCDF metadata for one variable}
  \begin{lstlisting}
  dimensions:
  longitude = 480 ;
  latitude = 241 ;
  time = UNLIMITED ; // (1096 currently)
  variables:
  float longitude(longitude) ;
  longitude:units = "degrees_east" ;
  longitude:long_name = "longitude" ;
  float latitude(latitude) ;
  latitude:units = "degrees_north" ;
  latitude:long_name = "latitude" ;
  int time(time) ;
  time:units = "hours since 1900-01-01 00:00:0.0" ;
  time:long_name = "time" ;
  time:calendar = "gregorian" ;
  short sund(time, latitude, longitude) ;
  sund:scale_factor = 0.659209863732776 ;
  sund:add_offset = 21599.6703950681 ;
  sund:_FillValue = -32767s ;
  sund:missing_value = -32767s ;
  sund:units = "s" ;
  sund:long_name = "Sunshine duration" ;

  // global attributes:
  :Conventions = "CF-1.0" ;
  :history = "2015-06-03 08:02:17 GMT by grib_to_netcdf-1.13.1: grib_to_netcdf /data/data04/scratch/netcdf-atls14-a562cefde8a29a7288fa0b8b7f9413f7-lFD4z9.target -o /data/data04/scratch/netcdf-atls14-a562cefde8a29a7288fa0b8b7f9413f7-CyGl1B.nc -utime" ;
  }
  \end{lstlisting}
\end{tcbcode}

To simplify search and identify data clearly, data services such as the WDCC and CERA, that offer data to the community, request scientists to provide additional metadata.
Normally, such data is provided when the results of an experiment is ingested into such a database.
Example metadata is listed in \Cref{tbl:additionalMetadata}.
In existing databases, the listed metadata is split into several fields, e.g. an address and email for persons, for simplicity only a rough overview is given.
Instead of encoding the history as a simple text field, it could
indicate detailed steps including the arguments for the commands and versions and transformations to reproduce the data.
This should include for each step, where and the ti
    #The history for the inputs, if the data lineage must be embedded
    ObjectId(<LONID>) : [
      # Assume LONID does not exist any more
    ],
  ]
},
"permissions" : {
  "UID"  : 1012,
  "GID"  : 400,
  "group" : "w", # allows read also
  "other" : "r"
},
"references" : {
  [ all links to used object IDs ]
},
"shards" : [
  ObjectId(<SHARD1 ID>),
  # For a sealed object, the domains of its shards can optionally be embedded:
  { "reference" : ObjectId(<SHARD2 ID>), "storage" : ... , "domain" },
  ObjectId(<SHARD3 ID>),
  ObjectId(<SHARD4 ID>)
]
\end{lstlisting}
\end{tcbcode}

The variable is split into multiple shards; metadata for one of them is shown in \Cref{lst:mongotemperatureshard}.
Since we assume domain decomposition in the application, the longitude and latitude variables are now only partially stored in a shard.
In the example, we assume two processes create one shard each and the surface of the earth is partitioned into four non-overlapping rectangles.

\begin{tcbcode}[label={lst:mongotemperatureshard}]{JSON document for a shard of the temperature variable}
\begin{lstlisting}
"_id" : ObjectId("<SHARD1 ID>"),
"sealed" : true,
"variable" : ObjectId("<TEMPID>"),
"type" : "float",
"domain" : {
    "longitude" : [ "min" : 0, "max" : 179999, "reference" : ObjectId("..") ],
    "latitude" : [ "min" : 0, "max" : 89999, "reference" : ObjectId("..") ],
    "time" : [ datetime(...), datetime(...), ... ]
  },
"storage" : {
    "plugin" : "pfs",
    "options" : {
      "path" : "/mnt/lustre/testdir/file1",
    },
    "serialization" : "row-major"
  },
"references : [
  ObjectId("<TEMPID>"),
  ObjectId(".."),
  ObjectId("..")
]
\end{lstlisting}
\end{tcbcode}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Data/Metadata Backend Drivers}
A prototypical metadata backend will be realized using MongoDB.
Advantages of using MongoDB are that it scales horizontally with the number of servers, provides fault-tolerance and that the document model supports arbitrary schemas.



To illustrate the applied mapping, we use a subset of our NetCDF metadata described in \Cref{sec:netcdfDataMapping}.
The excerpt is given in \Cref{lst:NetCDF-data-map}.
The mapping of a single logical variable is exemplarily described in


\begin{tcbcode}[label={lst:NetCDF-data-map}]{NetCDF metadata for one variable}
\begin{lstlisting}
dimensions:
	longitude = 480 ;
	latitude = 241 ;
	time = UNLIMITED ; // (1096 currently)
variables:
	float longitude(longitude) ;
		longitude:units = "degrees_east" ;
		longitude:long_name = "longitude" ;
	float latitude(latitude) ;
		latitude:units = "degrees_north" ;
		latitude:long_name = "latitude" ;
	int time(time) ;
		time:units = "hours since 1900-01-01 00:00:0.0" ;
		time:long_name = "time" ;
		time:calendar = "gregorian" ;
	short sund(time, latitude, longitude) ;
		sund:scale_factor = 0.659209863732776 ;
		sund:add_offset = 21599.6703950681 ;
		sund:_FillValue = -32767s ;
		sund:missing_value = -32767s ;
		sund:units = "s" ;
		sund:long_name = "Sunshine duration" ;

// global attributes:
		:Conventions = "CF-1.0" ;
		:history = "2015-06-03 08:02:17 GMT by grib_to_netcdf-1.13.1: grib_to_netcdf /data/data04/scratch/netcdf-atls14-a562cefde8a29a7288fa0b8b7f9413f7-lFD4z9.target -o /data/data04/scratch/netcdf-atls14-a562cefde8a29a7288fa0b8b7f9413f7-CyGl1B.nc -utime" ;
}
\end{lstlisting}
\end{tcbcode}

To simplify search and identify data clearly, data services such as the WDCC and CERA, that offer data to the community, request scientists to provide additional metadata.
Normally, such data is provided when the results of an experiment is ingested into such a database.
Example metadata is listed in \Cref{tbl:additionalMetadata}.
In existing databases, the listed metadata is split into several fields, e.g. an address and email for persons, for simplicity only a rough overview is given.
Instead of encoding the history as a simple text field, it could
indicate detailed steps including the arguments for the commands and versions and transformations to reproduce the data.
This should include for each step, where and the time when it is performed, and the versions of software used.

It is easily imaginable that most of this information could be useful already when the data is created as it simplifies the search and data management on the online storage.
Some of the data fields become only available after the initial data creation, e.g., the DOI.
Potentially the data must be updated / curated after the data is created.

\begin{table}
\begin{tabular}{ll}
Metadata & Description\\
\hline
Project & The scientific project during which the data is created \\
Institute & The institution which conducted the experiment\\
Person &  A natural person; could be a contact, running the experiment \\
Contact & Reference to person or consortium \\
DOI      & A document object identifier; useful for identifying a data publication\\
Topic     & Some information about the topic of the data / experiment \\
Experiment & Description of this particular experiment \\
History & A list with the history and transformations conducted with the data \\
\end{tabular}
\caption{Excerpt of additional scientific metadata}
\label{tbl:additionalMetadata}
\end{table}
